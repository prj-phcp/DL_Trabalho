{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 19 14:55:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   24C    P0    38W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Dispositivo onde tensores serão criados, armazenados e processados\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Randon Seed fixa para resultados reprodutíveis\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WellLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path, \n",
    "                 wells, \n",
    "                 var_in, \n",
    "                 var_out,\n",
    "                 normalizing_percentile=90.0,\n",
    "                 normalizing_split=0.2,\n",
    "                 normalizer=RobustScaler,\n",
    "                 max_sequence=16, \n",
    "                 step=1):\n",
    "        \n",
    "        self.path = path\n",
    "        with open(self.path + '/metadata.json', 'r') as metafile:\n",
    "            self.metadata = json.loads(metafile.read())\n",
    "        self.wells = wells\n",
    "        self.var_in = var_in\n",
    "        self.var_out = var_out\n",
    "        self.normalizing_percentile=normalizing_percentile\n",
    "        self.normalizing_split = normalizing_split\n",
    "        self.normalizer = normalizer\n",
    "        self.max_sequence = max_sequence\n",
    "        self.step = step\n",
    "        self.batches_X = None\n",
    "        self.batches_Y = None\n",
    "        self.outputs = None\n",
    "        self.normalizers = []\n",
    "        \n",
    "        indexes = self.get_wells_index(self.wells)\n",
    "        self.load_data_by_index(indexes)\n",
    "        \n",
    "    def get_wells_index(self, wells):\n",
    "        \n",
    "        indexes = []\n",
    "        for well, filt in wells:\n",
    "            indexes.extend([(meta['INDEX'], filt) for meta in self.metadata if meta['WELL'] == well])\n",
    "        return indexes\n",
    "    \n",
    "    def load_data_by_index(self, indexes):\n",
    "        \n",
    "        batches_X = []\n",
    "        batches_Y = []\n",
    "        outputs = []\n",
    "        for index, filt in indexes:\n",
    "            data = pd.read_json(f'{self.path}/{index}.json')#.reset_index()\n",
    "            # Armengue: Por liq vol para preencher o dataset\n",
    "            data['BORE_LIQ_VOL'] = data['BORE_OIL_VOL'] + data['BORE_WAT_VOL']\n",
    "            data = data[self.var_in + self.var_out].dropna().reset_index(drop=True)\n",
    "            X = data[self.var_in].values[filt,:]\n",
    "            Y = data[self.var_out].values[filt,:]\n",
    "            X_base, _, Y_base, _ = train_test_split(X, Y, test_size = self.normalizing_split)\n",
    "            #scaler_X = X_base.max(axis=0, keepdims=True)\n",
    "            #scaler_Y = Y_base.max(axis=0, keepdims=True)\n",
    "            #scaler_X = self.normalizer().fit(X_base)\n",
    "            #scaler_Y = self.normalizer().fit(Y_base)\n",
    "            scaler_X = np.percentile(X_base,self.normalizing_percentile,axis=0,keepdims=True)\n",
    "            scaler_Y = np.percentile(Y_base,self.normalizing_percentile,axis=0,keepdims=True)\n",
    "            self.normalizers.append((scaler_X, scaler_Y))\n",
    "            #X, Y = scaler_X.transform(X), scaler_Y.transform(Y)\n",
    "            X, Y = X / scaler_X, Y / scaler_Y\n",
    "            X, Y = torch.from_numpy(X.astype('float32')), torch.from_numpy(Y.astype('float32'))\n",
    "            output = Y[self.max_sequence::self.step]\n",
    "            #print(Y.shape)\n",
    "            #X = torch.split(X, self.max_sequence, dim= 0)\n",
    "            #Y = torch.split(Y, self.max_sequence, dim= 0)\n",
    "            X = X.unfold(0,self.max_sequence, self.step)\n",
    "            Y = Y.unfold(0,self.max_sequence, self.step)\n",
    "            batches_X.append(X[:-1,:,:])\n",
    "            batches_Y.append(Y[:-1,:,:])\n",
    "            outputs.append(Y[1:,:,:])\n",
    "            #print(X.shape)\n",
    "        self.batches_X = torch.concat(batches_X, axis=0)\n",
    "        self.batches_Y = torch.concat(batches_Y, axis=0)\n",
    "        self.outputs = torch.concat(outputs, axis=0)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.outputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                        \n",
    "        srcs = self.batches_X[idx,:,:]\n",
    "        trgts = self.batches_Y[idx,:,:]\n",
    "        output = self.outputs[idx,:,:]\n",
    "        \n",
    "        return srcs.permute(1,0), trgts.permute(1,0), output.permute(1,0)\n",
    "        \n",
    "        \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data for training :5424\n"
     ]
    }
   ],
   "source": [
    "path = './dataset/volve'\n",
    "wells = [\n",
    "    ('15/9-F-11', slice(15, None)),\n",
    "    ('15/9-F-12', slice(None, 800)),\n",
    "    ('15/9-F-14', slice(200, None)),\n",
    "    ('15/9-F-15 D', slice(10, 900)),\n",
    "    #('15/9-F-5', slice(None, None)),\n",
    "][::-1]\n",
    "\n",
    "#wells = [\n",
    "#    ('15/9-F-11', slice(15, 600)),\n",
    "#    ('15/9-F-12', slice(None, 800)),\n",
    "#    ('15/9-F-14', slice(1900, None)),\n",
    "#    ('15/9-F-15 D', slice(10, 900)),\n",
    "#    #('15/9-F-5', slice(None, None)),\n",
    "#][::-1]\n",
    "\n",
    "var_in = [\n",
    "        'AVG_DOWNHOLE_PRESSURE',\n",
    "        'AVG_WHP_P',\n",
    "        'AVG_CHOKE_SIZE_P',\n",
    "        'AVG_WHT_P',\n",
    "        'AVG_DOWNHOLE_TEMPERATURE',\n",
    "]\n",
    "\n",
    "var_out = [\n",
    "        #'BORE_OIL_VOL',\n",
    "        'BORE_LIQ_VOL',\n",
    "        #'BORE_GAS_VOL',\n",
    "        #'BORE_WAT_VOL',\n",
    "]\n",
    "\n",
    "\n",
    "dataset = WellLoader(path, wells, var_in, var_out, max_sequence = 16)\n",
    "print(f'Total data for training :{len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data for testing :699\n"
     ]
    }
   ],
   "source": [
    "path = './dataset/volve'\n",
    "wells = [\n",
    "    ('15/9-F-1 C', slice(28, None)),\n",
    "]\n",
    "\n",
    "var_in = [\n",
    "        'AVG_DOWNHOLE_PRESSURE',\n",
    "        'AVG_WHP_P',\n",
    "        'AVG_CHOKE_SIZE_P',\n",
    "        'AVG_WHT_P',\n",
    "        'AVG_DOWNHOLE_TEMPERATURE',\n",
    "]\n",
    "\n",
    "var_out = [\n",
    "        #'BORE_OIL_VOL',\n",
    "        'BORE_LIQ_VOL',\n",
    "        #'BORE_GAS_VOL',\n",
    "        #'BORE_WAT_VOL',\n",
    "]\n",
    "\n",
    "\n",
    "test_dataset = WellLoader(path, wells, var_in, var_out, max_sequence = 16)\n",
    "print(f'Total data for testing :{len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubsetSplitter:\n",
    "    \n",
    "    def __init__(self, batch_size, validation_split, test_split, shuffle=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.test_split = test_split\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __call__(self, dataset:Dataset):\n",
    "        \n",
    "        dataset_size = len(dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        validation_split = int(np.floor(self.validation_split * dataset_size))\n",
    "        test_split = int(np.floor(self.test_split * dataset_size))\n",
    "        train_split = dataset_size - validation_split - test_split\n",
    "        # Extracting test independently of others\n",
    "        test_indices = indices[train_split + validation_split:]\n",
    "        indices = indices[:train_split + validation_split]\n",
    "        if self.shuffle :\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices = indices[:train_split]\n",
    "        validation_indices = indices[train_split:]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(validation_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, \n",
    "                                                   sampler=train_sampler)\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size,\n",
    "                                                    sampler=valid_sampler)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size,\n",
    "                                                    sampler=test_sampler)\n",
    "        \n",
    "        return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = SubsetSplitter(16, 0.25, 0.05)\n",
    "train_loader, validation_loader, test_loader = splitter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4286, 0.0000, 0.0000,\n",
       "          0.0000, 1.4286]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "positional_encoding = PositionalEncoding(dim_model = 10, dropout_p=0.3, max_len=1200)\n",
    "tensor = torch.zeros((1,2,10))\n",
    "tensor\n",
    "encoded = positional_encoding(tensor)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs,\n",
    "        src_dim,\n",
    "        tgt_dim,\n",
    "        src_heads,\n",
    "        tgt_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "        dim_feedforward =2048,\n",
    "        num_linear_layers=0,\n",
    "        norm_first=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.src_dim = src_dim\n",
    "        self.tgt_dim = tgt_dim\n",
    "\n",
    "        # LAYERS\n",
    "        \n",
    "        # MONTANDO O TRANSFORMER\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=src_dim,\n",
    "            nhead=src_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            norm_first=norm_first\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer = encoder_layer,\n",
    "            num_layers = num_encoder_layers,\n",
    "            norm=None,\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=tgt_dim,\n",
    "            nhead=tgt_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            norm_first=norm_first\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer = decoder_layer,\n",
    "            num_layers = num_decoder_layers,\n",
    "            norm=None,\n",
    "        )\n",
    "            \n",
    "        # Como encoder e decoder podem ter dimenoes\n",
    "        self.memory_match = nn.Linear(src_dim, tgt_dim)\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(num_linear_layers):\n",
    "            self.linear_layers.append(nn.Linear(dim_model, dim_model))\n",
    "            self.linear_layers.append(nn.ReLU6())\n",
    "        self.out = nn.Linear(tgt_dim, num_outputs)\n",
    "        \n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None, is_causal=False):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Step1 - Passing source through encoder\n",
    "        memory = self.encoder(src, \n",
    "                              mask=src_mask,\n",
    "                              src_key_padding_mask=src_pad_mask,\n",
    "                              is_causal=is_causal)\n",
    "        \n",
    "        # Step2 - Matching memory sahpe to tgt shape\n",
    "        memory = self.memory_match(memory)\n",
    "        \n",
    "        \n",
    "        # Step3 - Passing tgt and memory through decoder\n",
    "        transformer_out = self.decoder(tgt, \n",
    "                                       memory,\n",
    "                                       tgt_mask=tgt_mask,\n",
    "                                       memory_mask=src_mask,\n",
    "                                       tgt_key_padding_mask=tgt_pad_mask,\n",
    "                                       memory_key_padding_mask=src_pad_mask,\n",
    "                                      )\n",
    "        \n",
    "        \n",
    "        for linear in self.linear_layers:\n",
    "            transformer_out = linear(transformer_out)\n",
    "        out = transformer_out\n",
    "        #out = torch.add(out, tgt)\n",
    "        \n",
    "        #recuperando informacao de escala\n",
    "        #out = torch.mul(out, tgt)\n",
    "        #bias = self.bias_layer(tgt)\n",
    "        #out = torch.add(out, bias)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \n",
    "    def __init__(self, epochs, loss, optimizer, scheduler, path, model_name='Transformer', model_size=8, early_stop=True, patience=5):\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.epochs = epochs\n",
    "        self.path = path\n",
    "        self.model_name = model_name\n",
    "        self.model_size = model_size\n",
    "        self.early_stop_flag = early_stop\n",
    "        self.patience = patience\n",
    "        self.clear_results()\n",
    "        \n",
    "    def clear_results(self):\n",
    "        \n",
    "        self.results = {\n",
    "            'Train':[],\n",
    "            'Validation':[],\n",
    "            'Test':[],\n",
    "        }\n",
    "        \n",
    "    def fit(self, model, train_loader, validation_loader, test_loader):\n",
    "        \n",
    "        self.clear_results()\n",
    "        torch.cuda.empty_cache()\n",
    "        decrease = self.patience\n",
    "        not_improved = 0\n",
    "        \n",
    "        model.to(device)\n",
    "        fit_time = time.time()\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            since = time.time()\n",
    "            running_loss = 0\n",
    "            #training loop\n",
    "            model.train()\n",
    "            self.train_loop(model, train_loader)\n",
    "            model.eval()\n",
    "            self.validation_loop(model, validation_loader)\n",
    "            self.test_loop(model, test_loader)\n",
    "            decrease, not_improved = self.early_stopping(validation_loader, decrease)\n",
    "            if not_improved == 1 and self.early_stop_flag:\n",
    "                print('[***] end training ...') \n",
    "                break\n",
    "            loss_per_training_batch = self.results['Train'][-1]\n",
    "            loss_per_validation_batch = self.results['Validation'][-1]\n",
    "            loss_per_test_batch = self.results['Test'][-1]\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, self.epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(loss_per_training_batch),\n",
    "                  \"Val Loss: {:.3f}..\".format(loss_per_validation_batch),\n",
    "                  \"Test Loss: {:.3f}..\".format(loss_per_test_batch),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "        \n",
    "    def train_loop(self, model, train_loader):\n",
    "        \n",
    "        mask = model.get_tgt_mask(self.model_size).to(device)\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            X, y_tgt, y_out = data\n",
    "            X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "            \n",
    "            output = model(X, y_tgt, src_mask=mask, tgt_mask=mask)\n",
    "            #print(X.shape, y_tgt.shape, y_out.shape, output.shape)\n",
    "            loss = self.loss(output[None,-1,:], y_out[None,-1,:])\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            self.optimizer.step() #update weight          \n",
    "            self.optimizer.zero_grad() #reset gradient\n",
    "            \n",
    "            #step the learning rate\n",
    "            if not self.scheduler is None:\n",
    "                self.scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        self.results['Train'].append(running_loss/len(train_loader))\n",
    "    \n",
    "    \n",
    "    def validation_loop(self, model, validation_loader):\n",
    "        \n",
    "        mask = model.get_tgt_mask(self.model_size).to(device)\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(validation_loader)):\n",
    "                #training phase\n",
    "                X, y_tgt, y_out = data\n",
    "                X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "                \n",
    "                output = model(X, y_tgt, src_mask=mask, tgt_mask=mask)\n",
    "                loss = self.loss(output[None,-1,:], y_out[None,-1,:])\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        self.results['Validation'].append(running_loss/len(validation_loader))\n",
    "        \n",
    "    def test_loop(self, model, test_loader):\n",
    "        \n",
    "        mask = model.get_tgt_mask(self.model_size).to(device)\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(test_loader)):\n",
    "                #training phase\n",
    "                X, y_tgt, y_out = data\n",
    "                X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "                \n",
    "                output = model(X, y_tgt, src_mask=mask, tgt_mask=mask)\n",
    "                loss = self.loss(output[None,-1,:], y_out[None,-1,:])\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        self.results['Test'].append(running_loss/len(test_loader))\n",
    "        \n",
    "    def early_stopping(self, validation_loader, decrease):\n",
    "        \n",
    "        loss_per_validation_batch = self.results['Validation'][-1]\n",
    "        min_loss = np.min(self.results['Validation'][:-1] + [np.inf])\n",
    "        if min_loss >= self.results['Validation'][-1]:\n",
    "            print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, loss_per_validation_batch))\n",
    "            decrease = self.patience\n",
    "            print('saving model...')\n",
    "            torch.save(model, self.path + f'/{self.model_name}.pt')\n",
    "        else:\n",
    "            decrease -= 1\n",
    "        if decrease < 0:     \n",
    "                not_improved = 1\n",
    "        else:\n",
    "            not_improved = 0\n",
    "        return decrease, not_improved\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \n",
    "        model = torch.load(self.path + f'/{self.model_name}.pt')\n",
    "        return model\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSTransformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=5, out_features=20, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=20, out_features=5, bias=True)\n",
      "        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1, out_features=1, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1, out_features=1, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=1, out_features=20, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=20, out_features=1, bias=True)\n",
      "        (norm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (memory_match): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (linear_layers): ModuleList()\n",
      "  (out): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TSTransformer(\n",
    "    num_outputs=1,\n",
    "    src_dim=5,\n",
    "    tgt_dim=1,\n",
    "    src_heads=1,\n",
    "    tgt_heads=1,\n",
    "    num_encoder_layers=24,\n",
    "    num_decoder_layers=24,\n",
    "    dropout_p=0.1,\n",
    "    dim_feedforward =20,\n",
    "    num_linear_layers=0,\n",
    "    norm_first=False,\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "lr_ = 5e-4\n",
    "epoch = 1000\n",
    "weight_decay = 1e-4\n",
    "path = '.'\n",
    "model_name = 'TRANSPOSED-MULTI'\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr_)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr_)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_, weight_decay=weight_decay)\n",
    "sched = None\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr_, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "training = Training(epoch, loss, optimizer, sched, model_name=model_name, model_size = dataset.max_sequence, path=path, early_stop=True, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  8.11it/s]\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]/projetos/c5ef/venvs/dl_raul/lib64/python3.9/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "100%|██████████| 85/85 [00:02<00:00, 41.94it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. inf >> 0.214 \n",
      "saving model...\n",
      "Epoch:1/1000.. Train Loss: 0.175.. Val Loss: 0.214.. Test Loss: 0.202.. Time: 0.53m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.58it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 40.81it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 40.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/1000.. Train Loss: 0.158.. Val Loss: 0.222.. Test Loss: 0.223.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.32it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.27it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.214 >> 0.202 \n",
      "saving model...\n",
      "Epoch:3/1000.. Train Loss: 0.159.. Val Loss: 0.202.. Test Loss: 0.233.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  8.06it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.70it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 40.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/1000.. Train Loss: 0.159.. Val Loss: 0.206.. Test Loss: 0.199.. Time: 0.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.76it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.08it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.202 >> 0.196 \n",
      "saving model...\n",
      "Epoch:5/1000.. Train Loss: 0.172.. Val Loss: 0.196.. Test Loss: 0.165.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.88it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.79it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/1000.. Train Loss: 0.134.. Val Loss: 0.246.. Test Loss: 0.167.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.69it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.69it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7/1000.. Train Loss: 0.164.. Val Loss: 0.271.. Test Loss: 0.156.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.82it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 40.03it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/1000.. Train Loss: 0.164.. Val Loss: 0.235.. Test Loss: 0.129.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:31<00:00,  7.60it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 37.35it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/1000.. Train Loss: 0.175.. Val Loss: 0.223.. Test Loss: 0.153.. Time: 0.57m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  8.13it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 38.79it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 38.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/1000.. Train Loss: 0.183.. Val Loss: 0.223.. Test Loss: 0.168.. Time: 0.53m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.30it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.57it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/1000.. Train Loss: 0.175.. Val Loss: 0.219.. Test Loss: 0.192.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.29it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 40.07it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/1000.. Train Loss: 0.186.. Val Loss: 0.274.. Test Loss: 0.187.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.34it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.37it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/1000.. Train Loss: 0.177.. Val Loss: 0.256.. Test Loss: 0.138.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.43it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.36it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/1000.. Train Loss: 0.177.. Val Loss: 0.242.. Test Loss: 0.170.. Time: 0.51m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.38it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 41.72it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/1000.. Train Loss: 0.146.. Val Loss: 0.227.. Test Loss: 0.263.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.27it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.09it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/1000.. Train Loss: 0.165.. Val Loss: 0.233.. Test Loss: 0.142.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.22it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 40.64it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.196 >> 0.175 \n",
      "saving model...\n",
      "Epoch:17/1000.. Train Loss: 0.172.. Val Loss: 0.175.. Test Loss: 0.189.. Time: 0.53m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.47it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.08it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18/1000.. Train Loss: 0.169.. Val Loss: 0.248.. Test Loss: 0.183.. Time: 0.51m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:25<00:00,  9.51it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 41.77it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19/1000.. Train Loss: 0.178.. Val Loss: 0.294.. Test Loss: 0.155.. Time: 0.46m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.38it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.58it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20/1000.. Train Loss: 0.158.. Val Loss: 0.223.. Test Loss: 0.116.. Time: 0.51m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.24it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.40it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 38.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:21/1000.. Train Loss: 0.157.. Val Loss: 0.218.. Test Loss: 0.165.. Time: 0.53m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.91it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.73it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:22/1000.. Train Loss: 0.164.. Val Loss: 0.266.. Test Loss: 0.103.. Time: 0.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.35it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 41.81it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:23/1000.. Train Loss: 0.164.. Val Loss: 0.231.. Test Loss: 0.243.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.60it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 41.27it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:24/1000.. Train Loss: 0.188.. Val Loss: 0.240.. Test Loss: 0.117.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  8.01it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.32it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:25/1000.. Train Loss: 0.169.. Val Loss: 0.295.. Test Loss: 0.209.. Time: 0.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.73it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 37.84it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 38.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:26/1000.. Train Loss: 0.175.. Val Loss: 0.190.. Test Loss: 0.138.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.73it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.16it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 37.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:27/1000.. Train Loss: 0.158.. Val Loss: 0.244.. Test Loss: 0.182.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  7.95it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.59it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 37.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:28/1000.. Train Loss: 0.160.. Val Loss: 0.218.. Test Loss: 0.189.. Time: 0.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:31<00:00,  7.66it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.15it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:29/1000.. Train Loss: 0.167.. Val Loss: 0.243.. Test Loss: 0.197.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.88it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.51it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 37.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:30/1000.. Train Loss: 0.162.. Val Loss: 0.190.. Test Loss: 0.124.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:31<00:00,  7.66it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.95it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/1000.. Train Loss: 0.129.. Val Loss: 0.241.. Test Loss: 0.132.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:31<00:00,  7.66it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.09it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:32/1000.. Train Loss: 0.155.. Val Loss: 0.328.. Test Loss: 0.135.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  7.96it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.97it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:33/1000.. Train Loss: 0.150.. Val Loss: 0.263.. Test Loss: 0.190.. Time: 0.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.76it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.38it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:34/1000.. Train Loss: 0.176.. Val Loss: 0.206.. Test Loss: 0.183.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.78it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 38.50it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 38.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:35/1000.. Train Loss: 0.179.. Val Loss: 0.261.. Test Loss: 0.134.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.86it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.07it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.175 >> 0.171 \n",
      "saving model...\n",
      "Epoch:36/1000.. Train Loss: 0.186.. Val Loss: 0.171.. Test Loss: 0.191.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.71it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 38.80it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 40.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:37/1000.. Train Loss: 0.172.. Val Loss: 0.207.. Test Loss: 0.175.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.74it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 40.46it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:38/1000.. Train Loss: 0.149.. Val Loss: 0.259.. Test Loss: 0.203.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.68it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.00it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 40.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:39/1000.. Train Loss: 0.167.. Val Loss: 0.263.. Test Loss: 0.225.. Time: 0.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.79it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.92it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 38.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40/1000.. Train Loss: 0.152.. Val Loss: 0.269.. Test Loss: 0.219.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:30<00:00,  7.85it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 39.66it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:41/1000.. Train Loss: 0.165.. Val Loss: 0.202.. Test Loss: 0.124.. Time: 0.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.57it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.92it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:42/1000.. Train Loss: 0.173.. Val Loss: 0.268.. Test Loss: 0.149.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.61it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.92it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 43.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:43/1000.. Train Loss: 0.139.. Val Loss: 0.190.. Test Loss: 0.176.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.57it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 43.00it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:44/1000.. Train Loss: 0.171.. Val Loss: 0.219.. Test Loss: 0.201.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.65it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.48it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 43.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:45/1000.. Train Loss: 0.175.. Val Loss: 0.191.. Test Loss: 0.086.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.45it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 38.59it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 37.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:46/1000.. Train Loss: 0.183.. Val Loss: 0.243.. Test Loss: 0.188.. Time: 0.52m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:29<00:00,  8.05it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 37.62it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:47/1000.. Train Loss: 0.161.. Val Loss: 0.208.. Test Loss: 0.095.. Time: 0.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:28<00:00,  8.21it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 40.38it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 40.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:48/1000.. Train Loss: 0.178.. Val Loss: 0.253.. Test Loss: 0.189.. Time: 0.53m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.67it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 43.00it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:49/1000.. Train Loss: 0.143.. Val Loss: 0.218.. Test Loss: 0.142.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.61it/s]\n",
      "100%|██████████| 85/85 [00:02<00:00, 42.44it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.171 >> 0.160 \n",
      "saving model...\n",
      "Epoch:50/1000.. Train Loss: 0.173.. Val Loss: 0.160.. Test Loss: 0.164.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.61it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.81it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:51/1000.. Train Loss: 0.171.. Val Loss: 0.214.. Test Loss: 0.133.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.68it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 43.47it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 43.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:52/1000.. Train Loss: 0.161.. Val Loss: 0.205.. Test Loss: 0.217.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.64it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.80it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:53/1000.. Train Loss: 0.170.. Val Loss: 0.188.. Test Loss: 0.125.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.63it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.70it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:54/1000.. Train Loss: 0.159.. Val Loss: 0.233.. Test Loss: 0.118.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.55it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.85it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:55/1000.. Train Loss: 0.166.. Val Loss: 0.236.. Test Loss: 0.161.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.57it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 42.82it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 42.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:56/1000.. Train Loss: 0.153.. Val Loss: 0.225.. Test Loss: 0.193.. Time: 0.50m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:27<00:00,  8.66it/s]\n",
      " 76%|███████▋  | 65/85 [00:01<00:00, 42.73it/s]"
     ]
    }
   ],
   "source": [
    "training.fit(model, train_loader, validation_loader, test_loader)\n",
    "best_model = training.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(training.results['Train'], label='Train')\n",
    "ax.plot(training.results['Validation'], label='Validation')\n",
    "ax.plot(training.results['Test'], label='Test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and plotting classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OSAEvaluator:\n",
    "    \n",
    "    def evaluate_OSA(self, dataset, model):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        Y_real = []\n",
    "        Y_pred = []\n",
    "        for i, data in enumerate(tqdm(dataset)):\n",
    "            src, tgt, y_real = data\n",
    "            src, tgt, y_real = src.to(device), tgt.to(device), y_real.to(device)\n",
    "            y_pred = model(src, tgt)\n",
    "            Y_real.append(y_real[-1,:])\n",
    "            Y_pred.append(y_pred[-1,:])\n",
    "        \n",
    "        Y_real = torch.vstack(Y_real).cpu()\n",
    "        Y_pred = torch.vstack(Y_pred).cpu().detach()\n",
    "        print('********** OSA Evaluation summary **********')\n",
    "        print(f'OSA MSE: {mean_squared_error(Y_real, Y_pred)}')\n",
    "        print(f'OSA RMSE: {np.sqrt(mean_squared_error(Y_real, Y_pred))}')\n",
    "        print(f'OSA R2 score: {r2_score(Y_real, Y_pred)}')\n",
    "        print('********************************************')\n",
    "        return Y_real, Y_pred\n",
    "            \n",
    "class FSEvaluator:\n",
    "    \n",
    "    def evaluate_FS(self, dataset, model):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        Y_real = []\n",
    "        Y_pred = []\n",
    "        tgt_sim = None\n",
    "        for i, data in enumerate(tqdm(dataset)):\n",
    "            src, tgt, y_real = data\n",
    "            src, tgt, y_real = src.to(device), tgt.to(device), y_real.to(device)\n",
    "            if tgt_sim is None:\n",
    "                tgt_sim = tgt\n",
    "            y_pred = model(src, tgt_sim)\n",
    "            tgt_sim[:-1,:] = tgt_sim[1:,:].clone()\n",
    "            tgt_sim[-1,:] = y_pred[-1,:].clone()\n",
    "            Y_real.append(y_real[-1,:])\n",
    "            Y_pred.append(y_pred[-1,:])\n",
    "        \n",
    "        Y_real = torch.vstack(Y_real).cpu()\n",
    "        Y_pred = torch.vstack(Y_pred).cpu().detach()\n",
    "        print('*********** FS Evaluation summary **********')\n",
    "        print(f'FS MSE: {mean_squared_error(Y_real, Y_pred)}')\n",
    "        print(f'FS RMSE: {np.sqrt(mean_squared_error(Y_real, Y_pred))}')\n",
    "        print(f'FS R2 score: {r2_score(Y_real, Y_pred)}')\n",
    "        print('********************************************')\n",
    "        return Y_real, Y_pred\n",
    "    \n",
    "class Evaluator(OSAEvaluator, FSEvaluator):\n",
    "    \n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "Y_real, Y_pred_OSA = evaluator.evaluate_OSA(test_dataset, model)\n",
    "Y_real, Y_pred_FS = evaluator.evaluate_FS(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_real, label='Real')\n",
    "ax.plot(Y_pred_OSA, label='OSA')\n",
    "ax.plot(Y_pred_FS, label='FS')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "Y_real, Y_pred_OSA = evaluator.evaluate_OSA(test_dataset, best_model)\n",
    "Y_real, Y_pred_FS = evaluator.evaluate_FS(test_dataset, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_real, label='Real')\n",
    "ax.plot(Y_pred_OSA, label='OSA')\n",
    "ax.plot(Y_pred_FS, label='FS')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raul_dl",
   "language": "python",
   "name": "raul_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
