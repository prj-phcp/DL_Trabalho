{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 11 10:11:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    53W / 300W |    924MiB / 32768MiB |     15%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     27247      C   .../venvs/dl_raul/bin/python      921MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Dispositivo onde tensores serão criados, armazenados e processados\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Randon Seed fixa para resultados reprodutíveis\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WellLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path, \n",
    "                 wells, \n",
    "                 var_in, \n",
    "                 var_out,\n",
    "                 normalizing_percentile=90.0,\n",
    "                 normalizing_split=0.2,\n",
    "                 normalizer=RobustScaler,\n",
    "                 max_sequence=16, \n",
    "                 step=1):\n",
    "        \n",
    "        self.path = path\n",
    "        with open(self.path + '/metadata.json', 'r') as metafile:\n",
    "            self.metadata = json.loads(metafile.read())\n",
    "        self.wells = wells\n",
    "        self.var_in = var_in\n",
    "        self.var_out = var_out\n",
    "        self.normalizing_percentile=normalizing_percentile\n",
    "        self.normalizing_split = normalizing_split\n",
    "        self.normalizer = normalizer\n",
    "        self.max_sequence = max_sequence\n",
    "        self.step = step\n",
    "        self.batches_X = None\n",
    "        self.batches_Y = None\n",
    "        self.outputs = None\n",
    "        self.normalizers = []\n",
    "        \n",
    "        indexes = self.get_wells_index(self.wells)\n",
    "        self.load_data_by_index(indexes)\n",
    "        \n",
    "    def get_wells_index(self, wells):\n",
    "        \n",
    "        indexes = []\n",
    "        for well, filt in wells:\n",
    "            indexes.extend([(meta['INDEX'], filt) for meta in self.metadata if meta['WELL'] == well])\n",
    "        return indexes\n",
    "    \n",
    "    def load_data_by_index(self, indexes):\n",
    "        \n",
    "        batches_X = []\n",
    "        batches_Y = []\n",
    "        outputs = []\n",
    "        for index, filt in indexes:\n",
    "            data = pd.read_json(f'{self.path}/{index}.json')#.reset_index()\n",
    "            # Armengue: Por liq vol para preencher o dataset\n",
    "            data['BORE_LIQ_VOL'] = data['BORE_OIL_VOL'] + data['BORE_WAT_VOL']\n",
    "            data = data[self.var_in + self.var_out].dropna().reset_index(drop=True)\n",
    "            X = data[self.var_in].values[filt,:]\n",
    "            Y = data[self.var_out].values[filt,:]\n",
    "            X_base, _, Y_base, _ = train_test_split(X, Y, test_size = self.normalizing_split)\n",
    "            #scaler_X = X_base.max(axis=0, keepdims=True)\n",
    "            #scaler_Y = Y_base.max(axis=0, keepdims=True)\n",
    "            #scaler_X = self.normalizer().fit(X_base)\n",
    "            #scaler_Y = self.normalizer().fit(Y_base)\n",
    "            scaler_X = np.percentile(X_base,self.normalizing_percentile,axis=0,keepdims=True)\n",
    "            scaler_Y = np.percentile(Y_base,self.normalizing_percentile,axis=0,keepdims=True)\n",
    "            self.normalizers.append((scaler_X, scaler_Y))\n",
    "            #X, Y = scaler_X.transform(X), scaler_Y.transform(Y)\n",
    "            X, Y = X / scaler_X, Y / scaler_Y\n",
    "            X, Y = torch.from_numpy(X.astype('float32')), torch.from_numpy(Y.astype('float32'))\n",
    "            output = Y[self.max_sequence::self.step]\n",
    "            #print(Y.shape)\n",
    "            #X = torch.split(X, self.max_sequence, dim= 0)\n",
    "            #Y = torch.split(Y, self.max_sequence, dim= 0)\n",
    "            X = X.unfold(0,self.max_sequence, self.step)\n",
    "            Y = Y.unfold(0,self.max_sequence, self.step)\n",
    "            batches_X.append(X)\n",
    "            batches_Y.append(Y)\n",
    "            outputs.append(output)\n",
    "            #print(X.shape)\n",
    "        self.batches_X = torch.concat(batches_X, axis=0)\n",
    "        self.batches_Y = torch.concat(batches_Y, axis=0)\n",
    "        self.outputs = torch.concat(outputs, axis=0)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.outputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                        \n",
    "        srcs = self.batches_X[idx,:,:]\n",
    "        trgts = self.batches_Y[idx,:,:]\n",
    "        output = self.outputs[idx,:]\n",
    "        \n",
    "        return srcs, trgts, output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data for training :5424\n"
     ]
    }
   ],
   "source": [
    "path = './dataset/volve'\n",
    "wells = [\n",
    "    ('15/9-F-11', slice(15, None)),\n",
    "    ('15/9-F-12', slice(None, 800)),\n",
    "    ('15/9-F-14', slice(200, None)),\n",
    "    ('15/9-F-15 D', slice(10, 900)),\n",
    "    #('15/9-F-5', slice(None, None)),\n",
    "][::-1]\n",
    "\n",
    "#wells = [\n",
    "#    ('15/9-F-11', slice(15, 600)),\n",
    "#    ('15/9-F-12', slice(None, 800)),\n",
    "#    ('15/9-F-14', slice(1900, None)),\n",
    "#    ('15/9-F-15 D', slice(10, 900)),\n",
    "#    #('15/9-F-5', slice(None, None)),\n",
    "#][::-1]\n",
    "\n",
    "var_in = [\n",
    "        'AVG_DOWNHOLE_PRESSURE',\n",
    "        'AVG_WHP_P',\n",
    "        'AVG_CHOKE_SIZE_P',\n",
    "        'AVG_WHT_P',\n",
    "        'AVG_DOWNHOLE_TEMPERATURE',\n",
    "]\n",
    "\n",
    "var_out = [\n",
    "        #'BORE_OIL_VOL',\n",
    "        'BORE_LIQ_VOL',\n",
    "        #'BORE_GAS_VOL',\n",
    "        #'BORE_WAT_VOL',\n",
    "]\n",
    "\n",
    "\n",
    "dataset = WellLoader(path, wells, var_in, var_out, max_sequence = 16)\n",
    "print(f'Total data for training :{len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data for testing :699\n"
     ]
    }
   ],
   "source": [
    "path = './dataset/volve'\n",
    "wells = [\n",
    "    ('15/9-F-1 C', slice(28, None)),\n",
    "]\n",
    "\n",
    "var_in = [\n",
    "        'AVG_DOWNHOLE_PRESSURE',\n",
    "        'AVG_WHP_P',\n",
    "        'AVG_CHOKE_SIZE_P',\n",
    "        'AVG_WHT_P',\n",
    "        'AVG_DOWNHOLE_TEMPERATURE',\n",
    "]\n",
    "\n",
    "var_out = [\n",
    "        #'BORE_OIL_VOL',\n",
    "        'BORE_LIQ_VOL',\n",
    "        #'BORE_GAS_VOL',\n",
    "        #'BORE_WAT_VOL',\n",
    "]\n",
    "\n",
    "\n",
    "test_dataset = WellLoader(path, wells, var_in, var_out, max_sequence = 16)\n",
    "print(f'Total data for testing :{len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubsetSplitter:\n",
    "    \n",
    "    def __init__(self, batch_size, validation_split, test_split, shuffle=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.test_split = test_split\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __call__(self, dataset:Dataset):\n",
    "        \n",
    "        dataset_size = len(dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        validation_split = int(np.floor(self.validation_split * dataset_size))\n",
    "        test_split = int(np.floor(self.test_split * dataset_size))\n",
    "        train_split = dataset_size - validation_split - test_split\n",
    "        # Extracting test independently of others\n",
    "        test_indices = indices[train_split + validation_split:]\n",
    "        indices = indices[:train_split + validation_split]\n",
    "        if self.shuffle :\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices = indices[:train_split]\n",
    "        validation_indices = indices[train_split:]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(validation_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, \n",
    "                                                   sampler=train_sampler)\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size,\n",
    "                                                    sampler=valid_sampler)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size,\n",
    "                                                    sampler=test_sampler)\n",
    "        \n",
    "        return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = SubsetSplitter(16, 0.25, 0.05)\n",
    "train_loader, validation_loader, test_loader = splitter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4286, 0.0000, 0.0000,\n",
       "          0.0000, 1.4286]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "positional_encoding = PositionalEncoding(dim_model = 10, dropout_p=0.3, max_len=1200)\n",
    "tensor = torch.zeros((1,2,10))\n",
    "tensor\n",
    "encoded = positional_encoding(tensor)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "        num_linear_layers=0,\n",
    "        norm_first=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        #self.embedding = nn.Embedding(num_outputs, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p, \n",
    "            batch_first=True,\n",
    "            norm_first = norm_first\n",
    "        )\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(num_linear_layers):\n",
    "            self.linear_layers.append(nn.Linear(dim_model, dim_model))\n",
    "            self.linear_layers.append(nn.ReLU6())\n",
    "        self.out = nn.Linear(dim_model, num_outputs)\n",
    "        #self.bias_layer = nn.Linear(dim_model, dim_model)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        #src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        #tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src_corr = src #* math.sqrt(self.dim_model)\n",
    "        tgt_corr = tgt #* math.sqrt(self.dim_model)\n",
    "        #src_corr = self.positional_encoder(src_corr)\n",
    "        #tgt_corr = self.positional_encoder(tgt_corr)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        #src = src.permute(1,0,2)\n",
    "        #tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src_corr, tgt_corr, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        for linear in self.linear_layers:\n",
    "            transformer_out = linear(transformer_out)\n",
    "        out = transformer_out\n",
    "        out = torch.add(out, tgt)\n",
    "        \n",
    "        #recuperando informacao de escala\n",
    "        #out = torch.mul(out, tgt)\n",
    "        #bias = self.bias_layer(tgt)\n",
    "        #out = torch.add(out, bias)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \n",
    "    def __init__(self, epochs, loss, optimizer, scheduler, path, model_name='Transformer', early_stop=True, patience=5):\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.epochs = epochs\n",
    "        self.path = path\n",
    "        self.model_name = model_name\n",
    "        self.early_stop_flag = early_stop\n",
    "        self.patience = patience\n",
    "        self.clear_results()\n",
    "        \n",
    "    def clear_results(self):\n",
    "        \n",
    "        self.results = {\n",
    "            'Train':[],\n",
    "            'Validation':[],\n",
    "            'Test':[],\n",
    "        }\n",
    "        \n",
    "    def fit(self, model, train_loader, validation_loader, test_loader):\n",
    "        \n",
    "        self.clear_results()\n",
    "        torch.cuda.empty_cache()\n",
    "        decrease = self.patience\n",
    "        not_improved = 0\n",
    "        \n",
    "        model.to(device)\n",
    "        fit_time = time.time()\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            since = time.time()\n",
    "            running_loss = 0\n",
    "            #training loop\n",
    "            model.train()\n",
    "            self.train_loop(model, train_loader)\n",
    "            model.eval()\n",
    "            self.validation_loop(model, validation_loader)\n",
    "            self.test_loop(model, test_loader)\n",
    "            decrease, not_improved = self.early_stopping(validation_loader, decrease)\n",
    "            if not_improved == 1 and self.early_stop_flag:\n",
    "                print('[***] end training ...') \n",
    "                break\n",
    "            loss_per_training_batch = self.results['Train'][-1]\n",
    "            loss_per_validation_batch = self.results['Validation'][-1]\n",
    "            loss_per_test_batch = self.results['Test'][-1]\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, self.epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(loss_per_training_batch),\n",
    "                  \"Val Loss: {:.3f}..\".format(loss_per_validation_batch),\n",
    "                  \"Test Loss: {:.3f}..\".format(loss_per_test_batch),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "        \n",
    "    def train_loop(self, model, train_loader):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            X, y_tgt, y_out = data\n",
    "            X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "            #y_result, y_tgt = Y, torch.from_numpy(-1.0*np.ones(Y.shape).astype('float32')).to(device)\n",
    "            #y_tgt[:,:,1:] = Y[:,:,:-1]\n",
    "            #y_tgt[:,:,0] = 0.0\n",
    "            output = model(X, y_tgt)\n",
    "            loss = self.loss(output.ravel(), y_out.ravel())\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            self.optimizer.step() #update weight          \n",
    "            self.optimizer.zero_grad() #reset gradient\n",
    "            \n",
    "            #step the learning rate\n",
    "            if not self.scheduler is None:\n",
    "                self.scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        self.results['Train'].append(running_loss/len(train_loader))\n",
    "    \n",
    "    \n",
    "    def validation_loop(self, model, validation_loader):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(validation_loader)):\n",
    "                #training phase\n",
    "                X, y_tgt, y_out = data\n",
    "                X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "                #y_result, y_tgt = Y, torch.from_numpy(-1.0*np.ones(Y.shape).astype('float32')).to(device)\n",
    "                #y_tgt[:,:,1:] = Y[:,:,:-1]\n",
    "                #y_tgt[:,:,0] = 0.0\n",
    "                output = model(X, y_tgt)\n",
    "                loss = self.loss(output.ravel(), y_out.ravel())\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        self.results['Validation'].append(running_loss/len(validation_loader))\n",
    "        \n",
    "    def test_loop(self, model, test_loader):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(test_loader)):\n",
    "                #training phase\n",
    "                X, y_tgt, y_out = data\n",
    "                X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "                #y_result, y_tgt = Y, torch.from_numpy(-1.0*np.ones(Y.shape).astype('float32')).to(device)\n",
    "                #y_tgt[:,:,1:] = Y[:,:,:-1]\n",
    "                #y_tgt[:,:,0] = 0.0\n",
    "                output = model(X, y_tgt)\n",
    "                loss = self.loss(output.ravel(), y_out.ravel())\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        self.results['Test'].append(running_loss/len(test_loader))\n",
    "        \n",
    "    def early_stopping(self, validation_loader, decrease):\n",
    "        \n",
    "        loss_per_validation_batch = self.results['Validation'][-1]\n",
    "        min_loss = np.min(self.results['Validation'][:-1] + [np.inf])\n",
    "        if min_loss >= self.results['Validation'][-1]:\n",
    "            print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, loss_per_validation_batch))\n",
    "            decrease = self.patience\n",
    "            print('saving model...')\n",
    "            torch.save(model, self.path + f'/{self.model_name}.pt')\n",
    "        else:\n",
    "            decrease -= 1\n",
    "        if decrease < 0:     \n",
    "                not_improved = 1\n",
    "        else:\n",
    "            not_improved = 0\n",
    "        return decrease, not_improved\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \n",
    "        model = torch.load(self.path + f'/{self.model_name}.pt')\n",
    "        return model\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    num_outputs=1, dim_model=dataset.max_sequence, num_heads=2, \n",
    "    num_encoder_layers=12, num_decoder_layers=6, dropout_p=0.1, norm_first=False,num_linear_layers=0).to(device)\n",
    "\n",
    "lr_ = 5e-4\n",
    "epoch = 500\n",
    "weight_decay = 1e-4\n",
    "path = '.'\n",
    "model_name = 'ADIM-MULTI'\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr_)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=lr_, weight_decay=weight_decay)\n",
    "sched = None\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr_, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "training = Training(epoch, loss, optimizer, sched, model_name=model_name, path=path, early_stop=False, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 22.00it/s]\n",
      "100%|██████████| 85/85 [00:00<00:00, 153.23it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 149.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. inf >> 0.325 \n",
      "saving model...\n",
      "Epoch:1/500.. Train Loss: 0.339.. Val Loss: 0.325.. Test Loss: 0.194.. Time: 0.19m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 23.69it/s]\n",
      "100%|██████████| 85/85 [00:00<00:00, 151.69it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 149.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.325 >> 0.279 \n",
      "saving model...\n",
      "Epoch:2/500.. Train Loss: 0.249.. Val Loss: 0.279.. Test Loss: 0.153.. Time: 0.18m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 33/238 [00:01<00:09, 22.11it/s]"
     ]
    }
   ],
   "source": [
    "training.fit(model, train_loader, validation_loader, test_loader)\n",
    "best_model = training.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(training.results['Train'], label='Train')\n",
    "ax.plot(training.results['Validation'], label='Validation')\n",
    "ax.plot(training.results['Test'], label='Test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and plotting classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OSAEvaluator:\n",
    "    \n",
    "    def evaluate_OSA(self, dataset, model):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        Y_real = []\n",
    "        Y_pred = []\n",
    "        for i, data in enumerate(tqdm(dataset)):\n",
    "            src, tgt, y_real = data\n",
    "            src, tgt, y_real = src.to(device), tgt.to(device), y_real.to(device)\n",
    "            y_pred = model(src, tgt)\n",
    "            Y_real.append(y_real.reshape(1,-1))\n",
    "            Y_pred.append(y_pred)\n",
    "        \n",
    "        Y_real = torch.vstack(Y_real).cpu()\n",
    "        Y_pred = torch.vstack(Y_pred).cpu().detach()\n",
    "        print('********** OSA Evaluation summary **********')\n",
    "        print(f'OSA MSE: {mean_squared_error(Y_real, Y_pred)}')\n",
    "        print(f'OSA RMSE: {np.sqrt(mean_squared_error(Y_real, Y_pred))}')\n",
    "        print(f'OSA R2 score: {r2_score(Y_real, Y_pred)}')\n",
    "        print('********************************************')\n",
    "        return Y_real, Y_pred\n",
    "            \n",
    "class FSEvaluator:\n",
    "    \n",
    "    def evaluate_FS(self, dataset, model):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        Y_real = []\n",
    "        Y_pred = []\n",
    "        tgt_sim = None\n",
    "        for i, data in enumerate(tqdm(dataset)):\n",
    "            src, tgt, y_real = data\n",
    "            src, tgt, y_real = src.to(device), tgt.to(device), y_real.to(device)\n",
    "            if tgt_sim is None:\n",
    "                tgt_sim = tgt\n",
    "            else:\n",
    "                tgt_sim[:-1,:] = tgt_sim[1:,:]\n",
    "                tgt_sim[-1,:] = y_pred\n",
    "            y_pred = model(src, tgt_sim)\n",
    "            Y_real.append(y_real.reshape(1,-1))\n",
    "            Y_pred.append(y_pred)\n",
    "        \n",
    "        Y_real = torch.vstack(Y_real).cpu()\n",
    "        Y_pred = torch.vstack(Y_pred).cpu().detach()\n",
    "        print('*********** FS Evaluation summary **********')\n",
    "        print(f'FS MSE: {mean_squared_error(Y_real, Y_pred)}')\n",
    "        print(f'FS RMSE: {np.sqrt(mean_squared_error(Y_real, Y_pred))}')\n",
    "        print(f'FS R2 score: {r2_score(Y_real, Y_pred)}')\n",
    "        print('********************************************')\n",
    "        return Y_real, Y_pred\n",
    "    \n",
    "class Evaluator(OSAEvaluator, FSEvaluator):\n",
    "    \n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "Y_real, Y_pred_OSA = evaluator.evaluate_OSA(test_dataset, model)\n",
    "Y_real, Y_pred_FS = evaluator.evaluate_FS(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_real, label='Real')\n",
    "ax.plot(Y_pred_OSA, label='OSA')\n",
    "ax.plot(Y_pred_FS, label='FS')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "Y_real, Y_pred_OSA = evaluator.evaluate_OSA(test_dataset, best_model)\n",
    "Y_real, Y_pred_FS = evaluator.evaluate_FS(test_dataset, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_real, label='Real')\n",
    "ax.plot(Y_pred_OSA, label='OSA')\n",
    "ax.plot(Y_pred_FS, label='FS')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raul_dl",
   "language": "python",
   "name": "raul_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
