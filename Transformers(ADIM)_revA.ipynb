{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 11 10:11:08 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    39W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Dispositivo onde tensores serão criados, armazenados e processados\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Randon Seed fixa para resultados reprodutíveis\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WellLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path, \n",
    "                 wells, \n",
    "                 var_in, \n",
    "                 var_out,\n",
    "                 normalizing_percentile=90.0,\n",
    "                 normalizing_split=0.2,\n",
    "                 normalizer=RobustScaler,\n",
    "                 max_sequence=16, \n",
    "                 step=1):\n",
    "        \n",
    "        self.path = path\n",
    "        with open(self.path + '/metadata.json', 'r') as metafile:\n",
    "            self.metadata = json.loads(metafile.read())\n",
    "        self.wells = wells\n",
    "        self.var_in = var_in\n",
    "        self.var_out = var_out\n",
    "        self.normalizing_percentile=normalizing_percentile\n",
    "        self.normalizing_split = normalizing_split\n",
    "        self.normalizer = normalizer\n",
    "        self.max_sequence = max_sequence\n",
    "        self.step = step\n",
    "        self.batches_X = None\n",
    "        self.batches_Y = None\n",
    "        self.outputs = None\n",
    "        self.normalizers = []\n",
    "        \n",
    "        indexes = self.get_wells_index(self.wells)\n",
    "        self.load_data_by_index(indexes)\n",
    "        \n",
    "    def get_wells_index(self, wells):\n",
    "        \n",
    "        indexes = []\n",
    "        for well, filt in wells:\n",
    "            indexes.extend([(meta['INDEX'], filt) for meta in self.metadata if meta['WELL'] == well])\n",
    "        return indexes\n",
    "    \n",
    "    def load_data_by_index(self, indexes):\n",
    "        \n",
    "        batches_X = []\n",
    "        batches_Y = []\n",
    "        outputs = []\n",
    "        for index, filt in indexes:\n",
    "            data = pd.read_json(f'{self.path}/{index}.json')#.reset_index()\n",
    "            # Armengue: Por liq vol para preencher o dataset\n",
    "            data['BORE_LIQ_VOL'] = data['BORE_OIL_VOL'] + data['BORE_WAT_VOL']\n",
    "            data = data[self.var_in + self.var_out].dropna().reset_index(drop=True)\n",
    "            X = data[self.var_in].values[filt,:]\n",
    "            Y = data[self.var_out].values[filt,:]\n",
    "            X_base, _, Y_base, _ = train_test_split(X, Y, test_size = self.normalizing_split)\n",
    "            #scaler_X = X_base.max(axis=0, keepdims=True)\n",
    "            #scaler_Y = Y_base.max(axis=0, keepdims=True)\n",
    "            scaler_X = self.normalizer().fit(X_base)\n",
    "            scaler_Y = self.normalizer().fit(Y_base)\n",
    "            #scaler_X = np.percentile(X_base,self.normalizing_percentile,axis=0,keepdims=True)\n",
    "            #scaler_Y = np.percentile(Y_base,self.normalizing_percentile,axis=0,keepdims=True)\n",
    "            self.normalizers.append((scaler_X, scaler_Y))\n",
    "            X, Y = scaler_X.transform(X), scaler_Y.transform(Y)\n",
    "            #X, Y = X / scaler_X, Y / scaler_Y\n",
    "            X, Y = torch.from_numpy(X.astype('float32')), torch.from_numpy(Y.astype('float32'))\n",
    "            output = Y[self.max_sequence::self.step]\n",
    "            #print(Y.shape)\n",
    "            #X = torch.split(X, self.max_sequence, dim= 0)\n",
    "            #Y = torch.split(Y, self.max_sequence, dim= 0)\n",
    "            X = X.unfold(0,self.max_sequence, self.step)\n",
    "            Y = Y.unfold(0,self.max_sequence, self.step)\n",
    "            batches_X.append(X)\n",
    "            batches_Y.append(Y)\n",
    "            outputs.append(output)\n",
    "            #print(X.shape)\n",
    "        self.batches_X = torch.concat(batches_X, axis=0)\n",
    "        self.batches_Y = torch.concat(batches_Y, axis=0)\n",
    "        self.outputs = torch.concat(outputs, axis=0)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.outputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                        \n",
    "        srcs = self.batches_X[idx,:,:]\n",
    "        trgts = self.batches_Y[idx,:,:]\n",
    "        output = self.outputs[idx,:]\n",
    "        \n",
    "        return srcs, trgts, output\n",
    "        \n",
    "        \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './dataset/volve'\n",
    "wells = [\n",
    "    ('15/9-F-1 C', slice(28, None)),\n",
    "]\n",
    "\n",
    "var_in = [\n",
    "        'AVG_DOWNHOLE_PRESSURE',\n",
    "        'AVG_WHP_P',\n",
    "        'AVG_CHOKE_SIZE_P',\n",
    "        'AVG_WHT_P',\n",
    "        'AVG_DOWNHOLE_TEMPERATURE',\n",
    "]\n",
    "\n",
    "var_out = [\n",
    "        #'BORE_OIL_VOL',\n",
    "        'BORE_LIQ_VOL',\n",
    "        #'BORE_GAS_VOL',\n",
    "        #'BORE_WAT_VOL',\n",
    "]\n",
    "\n",
    "\n",
    "dataset = WellLoader(path, wells, var_in, var_out, max_sequence = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubsetSplitter:\n",
    "    \n",
    "    def __init__(self, batch_size, validation_split, test_split, shuffle=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.test_split = test_split\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __call__(self, dataset:Dataset):\n",
    "        \n",
    "        dataset_size = len(dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        validation_split = int(np.floor(self.validation_split * dataset_size))\n",
    "        test_split = int(np.floor(self.test_split * dataset_size))\n",
    "        train_split = dataset_size - validation_split - test_split\n",
    "        # Extracting test independently of others\n",
    "        test_indices = indices[train_split + validation_split:]\n",
    "        indices = indices[:train_split + validation_split]\n",
    "        if self.shuffle :\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices = indices[:train_split]\n",
    "        validation_indices = indices[train_split:]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(validation_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, \n",
    "                                                   sampler=train_sampler)\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size,\n",
    "                                                    sampler=valid_sampler)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size,\n",
    "                                                    sampler=test_sampler)\n",
    "        \n",
    "        return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = SubsetSplitter(4, 0.1, 0.3, shuffle=False)\n",
    "train_loader, validation_loader, test_loader = splitter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4286, 0.0000, 0.0000,\n",
       "          0.0000, 1.4286]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "positional_encoding = PositionalEncoding(dim_model = 10, dropout_p=0.3, max_len=1200)\n",
    "tensor = torch.zeros((1,2,10))\n",
    "tensor\n",
    "encoded = positional_encoding(tensor)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "        num_linear_layers=0,\n",
    "        norm_first=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        #self.embedding = nn.Embedding(num_outputs, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p, \n",
    "            batch_first=True,\n",
    "            norm_first = norm_first\n",
    "        )\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(num_linear_layers):\n",
    "            self.linear_layers.append(nn.Linear(dim_model, dim_model))\n",
    "            self.linear_layers.append(nn.ReLU6())\n",
    "        self.out = nn.Linear(dim_model, num_outputs)\n",
    "        #self.bias_layer = nn.Linear(dim_model, dim_model)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        #src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        #tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src_corr = src #* math.sqrt(self.dim_model)\n",
    "        tgt_corr = tgt #* math.sqrt(self.dim_model)\n",
    "        #src_corr = self.positional_encoder(src_corr)\n",
    "        #tgt_corr = self.positional_encoder(tgt_corr)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        #src = src.permute(1,0,2)\n",
    "        #tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src_corr, tgt_corr, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        for linear in self.linear_layers:\n",
    "            transformer_out = linear(transformer_out)\n",
    "        out = transformer_out\n",
    "        out = torch.add(out, tgt)\n",
    "        \n",
    "        #recuperando informacao de escala\n",
    "        #out = torch.mul(out, tgt)\n",
    "        #bias = self.bias_layer(tgt)\n",
    "        #out = torch.add(out, bias)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \n",
    "    def __init__(self, epochs, loss, optimizer, scheduler, path, model_name='Transformer', early_stop=True, patience=5):\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.epochs = epochs\n",
    "        self.path = path\n",
    "        self.model_name = model_name\n",
    "        self.early_stop_flag = early_stop\n",
    "        self.patience = patience\n",
    "        self.clear_results()\n",
    "        \n",
    "    def clear_results(self):\n",
    "        \n",
    "        self.results = {\n",
    "            'Train':[],\n",
    "            'Validation':[],\n",
    "            'Test':[],\n",
    "        }\n",
    "        \n",
    "    def fit(self, model, train_loader, validation_loader, test_loader):\n",
    "        \n",
    "        self.clear_results()\n",
    "        torch.cuda.empty_cache()\n",
    "        decrease = self.patience\n",
    "        not_improved = 0\n",
    "        \n",
    "        model.to(device)\n",
    "        fit_time = time.time()\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            since = time.time()\n",
    "            running_loss = 0\n",
    "            #training loop\n",
    "            model.train()\n",
    "            self.train_loop(model, train_loader)\n",
    "            model.eval()\n",
    "            self.validation_loop(model, validation_loader)\n",
    "            self.test_loop(model, test_loader)\n",
    "            decrease, not_improved = self.early_stopping(validation_loader, decrease)\n",
    "            if not_improved == 1 and self.early_stop_flag:\n",
    "                print('[***] end training ...') \n",
    "                break\n",
    "            loss_per_training_batch = self.results['Train'][-1]\n",
    "            loss_per_validation_batch = self.results['Validation'][-1]\n",
    "            loss_per_test_batch = self.results['Test'][-1]\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, self.epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(loss_per_training_batch),\n",
    "                  \"Val Loss: {:.3f}..\".format(loss_per_validation_batch),\n",
    "                  \"Test Loss: {:.3f}..\".format(loss_per_test_batch),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "        \n",
    "    def train_loop(self, model, train_loader):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            X, y_tgt, y_out = data\n",
    "            X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "            #y_result, y_tgt = Y, torch.from_numpy(-1.0*np.ones(Y.shape).astype('float32')).to(device)\n",
    "            #y_tgt[:,:,1:] = Y[:,:,:-1]\n",
    "            #y_tgt[:,:,0] = 0.0\n",
    "            output = model(X, y_tgt)\n",
    "            loss = self.loss(output.ravel(), y_out.ravel())\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            self.optimizer.step() #update weight          \n",
    "            self.optimizer.zero_grad() #reset gradient\n",
    "            \n",
    "            #step the learning rate\n",
    "            if not self.scheduler is None:\n",
    "                self.scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        self.results['Train'].append(running_loss/len(train_loader))\n",
    "    \n",
    "    \n",
    "    def validation_loop(self, model, validation_loader):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(validation_loader)):\n",
    "                #training phase\n",
    "                X, y_tgt, y_out = data\n",
    "                X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "                #y_result, y_tgt = Y, torch.from_numpy(-1.0*np.ones(Y.shape).astype('float32')).to(device)\n",
    "                #y_tgt[:,:,1:] = Y[:,:,:-1]\n",
    "                #y_tgt[:,:,0] = 0.0\n",
    "                output = model(X, y_tgt)\n",
    "                loss = self.loss(output.ravel(), y_out.ravel())\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        self.results['Validation'].append(running_loss/len(validation_loader))\n",
    "        \n",
    "    def test_loop(self, model, test_loader):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(test_loader)):\n",
    "                #training phase\n",
    "                X, y_tgt, y_out = data\n",
    "                X, y_tgt, y_out = X.to(device), y_tgt.to(device), y_out.to(device)\n",
    "                #y_result, y_tgt = Y, torch.from_numpy(-1.0*np.ones(Y.shape).astype('float32')).to(device)\n",
    "                #y_tgt[:,:,1:] = Y[:,:,:-1]\n",
    "                #y_tgt[:,:,0] = 0.0\n",
    "                output = model(X, y_tgt)\n",
    "                loss = self.loss(output.ravel(), y_out.ravel())\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        self.results['Test'].append(running_loss/len(test_loader))\n",
    "        \n",
    "    def early_stopping(self, validation_loader, decrease):\n",
    "        \n",
    "        loss_per_validation_batch = self.results['Validation'][-1]\n",
    "        min_loss = np.min(self.results['Validation'][:-1] + [np.inf])\n",
    "        if min_loss >= self.results['Validation'][-1]:\n",
    "            print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, loss_per_validation_batch))\n",
    "            decrease = self.patience\n",
    "            print('saving model...')\n",
    "            torch.save(model, self.path + f'/{self.model_name}.pt')\n",
    "        else:\n",
    "            decrease -= 1\n",
    "        if decrease < 0:     \n",
    "                not_improved = 1\n",
    "        else:\n",
    "            not_improved = 0\n",
    "        return decrease, not_improved\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \n",
    "        model = torch.load(self.path + f'/{self.model_name}.pt')\n",
    "        return model\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    num_outputs=1, dim_model=dataset.max_sequence, num_heads=2, \n",
    "    num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1, norm_first=False,num_linear_layers=0).to(device)\n",
    "\n",
    "lr_ = 5e-4\n",
    "epoch = 1000\n",
    "weight_decay = 1e-4\n",
    "path = '.'\n",
    "model_name = 'ADIM'\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr_)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr_)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_, weight_decay=weight_decay)\n",
    "sched = None\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr_, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "training = Training(epoch, loss, optimizer, sched, model_name=model_name, path=path, early_stop=False, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:02<00:00, 52.61it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 375.11it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 388.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. inf >> 0.492 \n",
      "saving model...\n",
      "Epoch:1/1000.. Train Loss: 0.663.. Val Loss: 0.492.. Test Loss: 0.477.. Time: 0.04m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.55it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 384.51it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 386.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.492 >> 0.343 \n",
      "saving model...\n",
      "Epoch:2/1000.. Train Loss: 0.389.. Val Loss: 0.343.. Test Loss: 0.357.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 72.53it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 382.43it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 386.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.343 >> 0.314 \n",
      "saving model...\n",
      "Epoch:3/1000.. Train Loss: 0.321.. Val Loss: 0.314.. Test Loss: 0.322.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 71.37it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 366.90it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 378.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/1000.. Train Loss: 0.304.. Val Loss: 0.326.. Test Loss: 0.310.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 71.79it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 373.70it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 377.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.314 >> 0.290 \n",
      "saving model...\n",
      "Epoch:5/1000.. Train Loss: 0.299.. Val Loss: 0.290.. Test Loss: 0.298.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 72.91it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 378.73it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 386.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/1000.. Train Loss: 0.270.. Val Loss: 0.299.. Test Loss: 0.285.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.75it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 380.18it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 385.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.290 >> 0.268 \n",
      "saving model...\n",
      "Epoch:7/1000.. Train Loss: 0.259.. Val Loss: 0.268.. Test Loss: 0.275.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.91it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 386.81it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 388.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.268 >> 0.256 \n",
      "saving model...\n",
      "Epoch:8/1000.. Train Loss: 0.249.. Val Loss: 0.256.. Test Loss: 0.262.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 74.55it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 379.27it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 387.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.256 >> 0.246 \n",
      "saving model...\n",
      "Epoch:9/1000.. Train Loss: 0.237.. Val Loss: 0.246.. Test Loss: 0.251.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.25it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 308.18it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 384.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.246 >> 0.235 \n",
      "saving model...\n",
      "Epoch:10/1000.. Train Loss: 0.238.. Val Loss: 0.235.. Test Loss: 0.240.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.92it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 376.82it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 386.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/1000.. Train Loss: 0.222.. Val Loss: 0.254.. Test Loss: 0.239.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.01it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 384.74it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 385.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.235 >> 0.211 \n",
      "saving model...\n",
      "Epoch:12/1000.. Train Loss: 0.223.. Val Loss: 0.211.. Test Loss: 0.219.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.27it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 366.16it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 370.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.211 >> 0.205 \n",
      "saving model...\n",
      "Epoch:13/1000.. Train Loss: 0.198.. Val Loss: 0.205.. Test Loss: 0.206.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.14it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 372.79it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 388.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.205 >> 0.194 \n",
      "saving model...\n",
      "Epoch:14/1000.. Train Loss: 0.189.. Val Loss: 0.194.. Test Loss: 0.209.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 74.55it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 382.42it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 384.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.194 >> 0.183 \n",
      "saving model...\n",
      "Epoch:15/1000.. Train Loss: 0.173.. Val Loss: 0.183.. Test Loss: 0.191.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.27it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 377.50it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 384.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.183 >> 0.179 \n",
      "saving model...\n",
      "Epoch:16/1000.. Train Loss: 0.173.. Val Loss: 0.179.. Test Loss: 0.189.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 73.72it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 378.92it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 387.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.179 >> 0.159 \n",
      "saving model...\n",
      "Epoch:17/1000.. Train Loss: 0.162.. Val Loss: 0.159.. Test Loss: 0.179.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 68.32it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 309.84it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 363.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.159 >> 0.157 \n",
      "saving model...\n",
      "Epoch:18/1000.. Train Loss: 0.159.. Val Loss: 0.157.. Test Loss: 0.174.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 64.72it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 319.56it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 322.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.157 >> 0.151 \n",
      "saving model...\n",
      "Epoch:19/1000.. Train Loss: 0.169.. Val Loss: 0.151.. Test Loss: 0.175.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 68.33it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 317.33it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 320.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.151 >> 0.151 \n",
      "saving model...\n",
      "Epoch:20/1000.. Train Loss: 0.157.. Val Loss: 0.151.. Test Loss: 0.169.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.45it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 316.14it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 322.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.151 >> 0.142 \n",
      "saving model...\n",
      "Epoch:21/1000.. Train Loss: 0.140.. Val Loss: 0.142.. Test Loss: 0.168.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 66.39it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 316.82it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 327.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.142 >> 0.138 \n",
      "saving model...\n",
      "Epoch:22/1000.. Train Loss: 0.142.. Val Loss: 0.138.. Test Loss: 0.164.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 69.03it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 322.27it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 304.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:23/1000.. Train Loss: 0.137.. Val Loss: 0.157.. Test Loss: 0.169.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 69.18it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 315.84it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 317.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:24/1000.. Train Loss: 0.139.. Val Loss: 0.166.. Test Loss: 0.161.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.75it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 319.43it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 325.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.138 >> 0.134 \n",
      "saving model...\n",
      "Epoch:25/1000.. Train Loss: 0.131.. Val Loss: 0.134.. Test Loss: 0.161.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 59.59it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 321.00it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 320.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:26/1000.. Train Loss: 0.130.. Val Loss: 0.137.. Test Loss: 0.156.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 65.63it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 323.16it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 300.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.134 >> 0.131 \n",
      "saving model...\n",
      "Epoch:27/1000.. Train Loss: 0.130.. Val Loss: 0.131.. Test Loss: 0.155.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.85it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 309.77it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 313.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:28/1000.. Train Loss: 0.123.. Val Loss: 0.134.. Test Loss: 0.152.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.93it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 282.33it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 302.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:29/1000.. Train Loss: 0.121.. Val Loss: 0.142.. Test Loss: 0.147.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 62.87it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 316.75it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 323.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.131 >> 0.124 \n",
      "saving model...\n",
      "Epoch:30/1000.. Train Loss: 0.127.. Val Loss: 0.124.. Test Loss: 0.149.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.96it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 321.10it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 324.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/1000.. Train Loss: 0.129.. Val Loss: 0.129.. Test Loss: 0.140.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 66.16it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 280.10it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 311.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:32/1000.. Train Loss: 0.121.. Val Loss: 0.129.. Test Loss: 0.140.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.81it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 321.04it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 317.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:33/1000.. Train Loss: 0.109.. Val Loss: 0.127.. Test Loss: 0.136.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 66.27it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 318.45it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 324.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. 0.124 >> 0.124 \n",
      "saving model...\n",
      "Epoch:34/1000.. Train Loss: 0.117.. Val Loss: 0.124.. Test Loss: 0.136.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 64.92it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 321.21it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 308.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:35/1000.. Train Loss: 0.114.. Val Loss: 0.126.. Test Loss: 0.130.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.73it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 325.05it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 323.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:36/1000.. Train Loss: 0.118.. Val Loss: 0.126.. Test Loss: 0.133.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 67.00it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 327.47it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 321.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:37/1000.. Train Loss: 0.112.. Val Loss: 0.130.. Test Loss: 0.128.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 64.74it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 322.61it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 323.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:38/1000.. Train Loss: 0.104.. Val Loss: 0.129.. Test Loss: 0.133.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 68.24it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 320.27it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 324.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:39/1000.. Train Loss: 0.112.. Val Loss: 0.124.. Test Loss: 0.133.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:01<00:00, 68.89it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 318.31it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 324.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40/1000.. Train Loss: 0.105.. Val Loss: 0.135.. Test Loss: 0.134.. Time: 0.03m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 79/107 [00:01<00:00, 67.13it/s]"
     ]
    }
   ],
   "source": [
    "training.fit(model, train_loader, validation_loader, test_loader)\n",
    "best_model = training.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(training.results['Train'], label='Train')\n",
    "ax.plot(training.results['Validation'], label='Validation')\n",
    "ax.plot(training.results['Test'], label='Test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and plotting classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OSAEvaluator:\n",
    "    \n",
    "    def evaluate_OSA(self, dataset, model):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        Y_real = []\n",
    "        Y_pred = []\n",
    "        for i, data in enumerate(tqdm(dataset)):\n",
    "            src, tgt, y_real = data\n",
    "            src, tgt, y_real = src.to(device), tgt.to(device), y_real.to(device)\n",
    "            y_pred = model(src, tgt)\n",
    "            Y_real.append(y_real.reshape(1,-1))\n",
    "            Y_pred.append(y_pred)\n",
    "        \n",
    "        Y_real = torch.vstack(Y_real).cpu()\n",
    "        Y_pred = torch.vstack(Y_pred).cpu().detach()\n",
    "        print('********** OSA Evaluation summary **********')\n",
    "        print(f'OSA MSE: {mean_squared_error(Y_real, Y_pred)}')\n",
    "        print(f'OSA RMSE: {np.sqrt(mean_squared_error(Y_real, Y_pred))}')\n",
    "        print(f'OSA R2 score: {r2_score(Y_real, Y_pred)}')\n",
    "        print('********************************************')\n",
    "        return Y_real, Y_pred\n",
    "            \n",
    "class FSEvaluator:\n",
    "    \n",
    "    def evaluate_FS(self, dataset, model):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        Y_real = []\n",
    "        Y_pred = []\n",
    "        tgt_sim = None\n",
    "        for i, data in enumerate(tqdm(dataset)):\n",
    "            src, tgt, y_real = data\n",
    "            src, tgt, y_real = src.to(device), tgt.to(device), y_real.to(device)\n",
    "            if tgt_sim is None:\n",
    "                tgt_sim = tgt\n",
    "            else:\n",
    "                tgt_sim[:-1,:] = tgt_sim[1:,:]\n",
    "                tgt_sim[-1,:] = y_pred\n",
    "            y_pred = model(src, tgt_sim)\n",
    "            Y_real.append(y_real.reshape(1,-1))\n",
    "            Y_pred.append(y_pred)\n",
    "        \n",
    "        Y_real = torch.vstack(Y_real).cpu()\n",
    "        Y_pred = torch.vstack(Y_pred).cpu().detach()\n",
    "        print('*********** FS Evaluation summary **********')\n",
    "        print(f'FS MSE: {mean_squared_error(Y_real, Y_pred)}')\n",
    "        print(f'FS RMSE: {np.sqrt(mean_squared_error(Y_real, Y_pred))}')\n",
    "        print(f'FS R2 score: {r2_score(Y_real, Y_pred)}')\n",
    "        print('********************************************')\n",
    "        return Y_real, Y_pred\n",
    "    \n",
    "class Evaluator(OSAEvaluator, FSEvaluator):\n",
    "    \n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "Y_real, Y_pred_OSA = evaluator.evaluate_OSA(dataset, model)\n",
    "Y_real, Y_pred_FS = evaluator.evaluate_FS(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_real, label='Real')\n",
    "ax.plot(Y_pred_OSA, label='OSA')\n",
    "ax.plot(Y_pred_FS, label='FS')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "Y_real, Y_pred_OSA = evaluator.evaluate_OSA(dataset, best_model)\n",
    "Y_real, Y_pred_FS = evaluator.evaluate_FS(dataset, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_real, label='Real')\n",
    "ax.plot(Y_pred_OSA, label='OSA')\n",
    "ax.plot(Y_pred_FS, label='FS')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raul_dl",
   "language": "python",
   "name": "raul_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
